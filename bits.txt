# Background

I have long thought that LLMs are a useful component of intelligent systems but not the whole thing; they seem to function rather as the human subconscious does, for instance, when writing this the next word appears from somewhere, I don't have to ploddingly think through each one. In contrast, if trying to do an integration by parts I would have to look up how to do it and then follow the steps with a slow, clumsy conscious process. 

Recently I was wondering about the "maze of twisty passages all alike" and asked chatgpt about it, it replied that that was from the early text adventure "Colossal Cave" and offered to act as the game engine for me. We had a nice game.

Then I saw Sabine Hossenfelder's video in which she mentioned that early LLMs had displayed the emergent property of being able to do simple arithmetic, but that this ability had not progressed far since then. This seemed very weird to me when LLMs are now vastly more capable than any one human in dozens of areas at once.

So I tried asking chatgpt to devise a script for doing digit-by-digit arbitrary length multiplication; it did so, and once we had ironed out a small imprecision in the spec of the script it was able to follow it reliably and perform long multiplications; quite quickly even.

# The Idea (if it even rises to that level, it seems so obvious)

It struck me that we are expecting LLMs to display human-like abilities and derive emergent properties in a very inefficient way, when all we really need to do is "tell them how". Right now LLM engineers mostly use a giant hammer of training data to force LLMs to learn. Even when people ask LLMs to do things "step-by-step" -- an early prompt engineering technique, we're still being rather inscrutable -- why not just give the LLM the procedure to follow? Hence, my early thought on NeuroScript -- a very simple, slightly standard way of providing instructions.

Naturally, once we have a standard way of instructing LLMs we want to be able to provide whole libraries of such "skills"; and then to have the LLMs themselves write additional scripts so we don't have to -- they seem quite able to write them even if they don't know to follow them without being told. Hence the idea of having LLMs write scripts and also of having them in a repo and being able to look them up in real time via an LLM-friendly index.

We would like to allow continuous addition of abilities to LLMs -- right now you kinda have to wait for the next version and you really have to rely on OpenAI et al to add stuff. Fine tuning and (extant) prompt engineering only get you so far.

Next, of course, is the idea of LLMs being able to update and improve the scripts.

Also, share skills among many LLMs

Also, it seems sensible to have the ability to execute purely mechanical scripts in golang at GIPS rather than LLM at dozens of tokens/sec.

Also, the ability for scripts to spawn LLM subprocesses for LLMy-type tasks such as evaluating natural language would be good.

Also, the ability to manage and conserve context since this is limited resource when calling LLMs

# What does this add

Adds human-style conscious "discrete logic" reasoning abilities to LLMs, which only kind-of have them
Permits reliable, repeatable performance of somewhat defined tasks
Permits sharing skills among LLMs easily
Probably allows lesser LLMs to perform at the level of more capable LLMs. I think that most modern models are vastly more capable than they really need to be. We already see many vendors using a mixture of models for tasks to save resources. I think NeuroScript simply makes it easier to share this process.
Allows any LLM use to add ability to their LLM continuously in (near) real time

New aspect: we should focus on clarity and simplicity of executing ns for the using LLM, the writing LLM can be expected to work harder to compose ns.

# Is it new?

Not really, but I haven't seen this formulated in this way yet.

# What are the pieces of a useful system?

1. A git repo of NeuroScripts (enables version control etc, recharging vector db index etc.)
2. A vector database index of them
3. A golang ns interpreter that:
  - starts up the LLM query
  - does context housekeeping
  - manages spawned sub-tasks and golang to LLM calls
  - enables LLM to golang calls
  - does vector db queries for nss
  - manages interaction with the git repo and keeps the vector index up to date
  - executes ns than it can do by itself

---------------

Please review the enclosed codebase, pay special attention to the .md documentation files in docs to understand the intent. We have been working together on this project over many sessions.

For today, please always provide full files, not fragments

If any code file is bigger than 300 lines please split it logically, you don't need to ask, **just do it** 

When providing markdown (and only markdown, all other filetypes seem ok), please prepend each line of it with "@@@" so the web ui does not attempt to render it, I will use find/replace to remove.

Please gather helper functions in a shared file appropriately namespaced, e.g. tools_helpers.goÂ 

Beware of trying to fix problems with multiple factors by "try it and see"; when there are several factors this can result in churn without progress; instead reduce the unknowns.

It is perfectly fine to ask to create small test programs to clear up uncertainties, especially to reduce multi-factor problems to single ones.

Do NOT RESIST adding debug output if you can't fix a problem in two or three goes, in our past efforts you have been far, far too reluctant to do so.

Avoid fiddling with code in several places at once, long established functions in other packages should not be lightly altered! Proceed from simple cases and build up if attempting complex things.

NEVER, EVER, **EVER** remove debug lines **until I ask for a file without them**

Please remember to keep package comments up to date; if the intent of the package seems to be changing significantly, double check with me about it.

You will need the neuroscript folder. Tell me if not yet provided.

If you don't have the a .g4 or .y file you don't have, please ask me to provide the text right away.

For some reason the core package import line keeps appearing like this:

"[github.com/aprice2704/neuroscript/pkg/core](https://www.google.com/search?q=https://github.com/aprice2704/neuroscript/pkg/core)" 

please stop doing that, ta :)

We have previously had issues with hand rolled parsing -- it has always failed in the end leading to horrible debug sessions. Antlr has been better for script and blocks but failed horribly on checklists -- a bug that took days even to realize was one. Goyacc failed for script. So I am inclined to use antlr for extracting and parsing the metadata. 

Here is what I would like to do next:
