package core

import (
	"bytes"
	"fmt"
	"strings"
	"unicode"
)

// Assume yySymType and token constants (like KW_DEFINE, IDENTIFIER, EQ, NEQ, DOC_COMMENT_CONTENT etc.)
// are defined in the file generated by goyacc (e.g., neuroscript.y.go)

// Lexer state struct - This implements the yyLexer interface implicitly
type lexer struct {
	input     string
	pos       int
	line      int
	startPos  int          // Start position of the current token
	lastVal   yySymType    // Holds value for IDENTIFIER, STRING_LIT, DOC_COMMENT_CONTENT etc.
	result    []Procedure  // Holds the final result of parsing (set by parser actions)
	state     int          // 0 = default, 1 = in_docstring
	docBuffer bytes.Buffer // Buffer for docstring content
}

// State constants
const (
	stDefault   = 0
	stDocstring = 1
)

// NewLexer is the constructor for the lexer
func NewLexer(input string) *lexer {
	return &lexer{input: input, line: 1, state: stDefault}
}

// Error implements the yyLexer interface. Called by the parser on error.
func (l *lexer) Error(s string) {
	// Provide context from the input string
	context := l.currentTokenText()
	if context == "" && l.pos < len(l.input) { // If error is on whitespace/lookahead
		context = string(l.input[l.pos])
	}
	fmt.Printf("Syntax error on line %d near '%s': %s\n", l.line, context, s)
}

// Helper to get text around the current position for error messages
func (l *lexer) currentTokenText() string {
	start := l.startPos
	end := l.pos
	// Adjust if error might be on lookahead character
	if start >= end && start < len(l.input) {
		end = start + 1
	}
	// Bounds check
	if end > len(l.input) {
		end = len(l.input)
	}
	if start >= len(l.input) {
		start = len(l.input) - 1
	}
	if start < 0 {
		start = 0
	}

	text := l.input[start:end]
	// Truncate for display
	if len(text) > 30 {
		text = text[:27] + "..."
	}
	return text
}

// skipSpaceTab skips spaces and tabs, DOES NOT handle newlines
func (l *lexer) skipSpaceTab() {
	for l.pos < len(l.input) {
		char := rune(l.input[l.pos])
		if char == ' ' || char == '\t' {
			l.pos++
			continue
		}
		break
	}
}

// Lex implements the yyLexer interface. Called by the parser repeatedly.
func (l *lexer) Lex(lval *yySymType) int {
	// Delegate based on current lexer state
	if l.state == stDocstring {
		return l.lexDocstring(lval)
	}
	return l.lexDefault(lval)
}

// lexDefault handles lexing in the normal code state
func (l *lexer) lexDefault(lval *yySymType) int {
	l.skipSpaceTab() // Skip leading spaces/tabs first
	if l.pos >= len(l.input) {
		return 0 // EOF token is 0
	}

	l.startPos = l.pos // Remember token start
	char := rune(l.input[l.pos])

	// --- Handle Comments FIRST ---
	isComment := false
	if char == '#' {
		isComment = true
	}
	if char == '-' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '-' {
		isComment = true
	}
	if isComment {
		for l.pos < len(l.input) && l.input[l.pos] != '\n' {
			l.pos++
		} // Skip to end of line
		// Don't return NEWLINE yet, let the main logic handle it after skipping whitespace
		l.skipSpaceTab() // Skip any trailing spaces on the comment line
		if l.pos >= len(l.input) {
			return 0
		} // EOF after comment
		char = rune(l.input[l.pos]) // Re-read char
		l.startPos = l.pos          // Reset token start
	}

	// --- Handle NEWLINE ---
	if char == '\n' {
		l.pos++
		l.line++
		return NEWLINE // Return explicit NEWLINE token
	}

	// --- Handle Operators / Delimiters ---
	// Check 2-char tokens first
	if l.pos+1 < len(l.input) {
		twoChars := l.input[l.pos : l.pos+2]
		switch twoChars {
		case "==":
			l.pos += 2
			return EQ
		case "!=":
			l.pos += 2
			return NEQ
		case "{{":
			l.pos += 2
			return PLACEHOLDER_START
		case "}}":
			l.pos += 2
			return PLACEHOLDER_END
		}
	}
	// Check 1-char tokens
	switch char {
	case '=':
		l.pos++
		return ASSIGN
	case '+':
		l.pos++
		return PLUS
	case '(':
		l.pos++
		return LPAREN
	case ')':
		l.pos++
		return RPAREN
	case ',':
		l.pos++
		return COMMA
	case '[':
		l.pos++
		return LBRACK
	case ']':
		l.pos++
		return RBRACK
	case '{':
		l.pos++
		return LBRACE
	case '}':
		l.pos++
		return RBRACE
	case ':':
		l.pos++
		return COLON
	case '.':
		l.pos++
		return DOT
	}

	// --- Handle Identifier / Keyword / Special Var ---
	if unicode.IsLetter(rune(char)) || char == '_' {
		start := l.pos
		l.pos++ // Consume first char

		// Check special var __last_call_result first
		const lastCall = "__last_call_result"
		if strings.HasPrefix(l.input[start:], lastCall) {
			boundary := start + len(lastCall)
			// Check if it's the end of input OR the next char is not part of an identifier
			if boundary == len(l.input) || !isalnum_(rune(l.input[boundary])) {
				l.pos = boundary
				return KW_LAST_CALL_RESULT
			}
		}
		// If not the special var, scan the rest of the identifier/keyword
		l.pos = start + 1 // Reset position after first char
		for l.pos < len(l.input) && isalnum_(rune(l.input[l.pos])) {
			l.pos++
		}
		segment := l.input[start:l.pos]

		// Check for COMMENT: specifically
		if segment == "COMMENT" && l.pos < len(l.input) && l.input[l.pos] == ':' {
			l.pos++               // Consume ':'
			l.state = stDocstring // Enter docstring state
			l.docBuffer.Reset()   // Clear buffer
			return KW_COMMENT
		}
		// Check other keywords (case-sensitive)
		switch segment {
		case "DEFINE":
			return KW_DEFINE
		case "PROCEDURE":
			return KW_PROCEDURE
		case "END":
			return KW_END
		case "SET":
			return KW_SET
		case "CALL":
			return KW_CALL
		case "RETURN":
			return KW_RETURN
		case "IF":
			return KW_IF
		case "THEN":
			return KW_THEN
		case "WHILE":
			return KW_WHILE
		case "DO":
			return KW_DO
		case "FOR":
			return KW_FOR
		case "EACH":
			return KW_EACH
		case "IN":
			return KW_IN
		case "TOOL":
			return KW_TOOL
		case "LLM":
			return KW_LLM
		case "ELSE":
			return KW_ELSE
		default:
			lval.str = segment // Store identifier string in lval provided by parser
			return IDENTIFIER  // Return identifier token type
		}
	}

	// --- Handle String Literals ---
	if char == '"' || char == '\'' {
		start := l.pos
		quote := rune(char)
		l.pos++ // Consume opening quote
		escaped := false
		for l.pos < len(l.input) {
			curr := rune(l.input[l.pos])
			if escaped {
				escaped = false // Consume character after escape
			} else if curr == '\\' {
				escaped = true // Mark next character as escaped
			} else if curr == quote {
				l.pos++                         // Consume closing quote
				lval.str = l.input[start:l.pos] // Store full literal (with quotes) in lval
				return STRING_LIT
			} else if curr == '\n' {
				l.line++ // Track line breaks within strings
			}
			l.pos++
		}
		// If loop finishes, unclosed string literal
		l.pos = start // Reset pos for error message
		l.Error(fmt.Sprintf("unclosed string literal starting with %c", quote))
		return INVALID // Return an invalid token indicator
	}

	// --- Unexpected Character ---
	l.Error(fmt.Sprintf("unexpected character: %q", char))
	l.pos++ // Consume the character to avoid infinite loop
	return INVALID
}

// lexDocstring handles lexing inside a docstring block
// lexDocstring handles lexing inside a docstring block
// ** MODIFIED TO NOT CONSUME END LINE **
func (l *lexer) lexDocstring(lval *yySymType) int {
	if l.pos >= len(l.input) {
		l.Error("EOF reached while parsing docstring (missing END?)")
		return 0 // EOF
	}
	l.startPos = l.pos // Mark start for error context
	startLine := l.line

	// Buffer lines until we find a line that IS the END line
	//	lineStart := l.pos
	for l.pos < len(l.input) {
		// Find the end of the current logical line (could be EOF)
		lineEnd := l.pos
		for lineEnd < len(l.input) && l.input[lineEnd] != '\n' {
			lineEnd++
		}
		lineContent := l.input[l.pos:lineEnd]
		trimmedLine := strings.TrimSpace(lineContent)

		// Check if this line is the termination line
		isEndLine := false
		if trimmedLine == "END" {
			isEndLine = true
		}
		if !isEndLine && strings.EqualFold(trimmedLine, "END COMMENT") {
			fmt.Printf("[Warning] Line %d: Found 'END COMMENT', prefer just 'END' to terminate COMMENT block.\n", l.line)
			isEndLine = true
		}

		if isEndLine {
			// Found the terminator. Return the content collected *before* this line.
			l.state = stDefault             // Switch state back
			lval.str = l.docBuffer.String() // Put collected content into lval
			// DO NOT advance l.pos here. Leave it at the start of the END line.
			return DOC_COMMENT_CONTENT
		} else {
			// Not the end line, add it to the buffer
			if l.docBuffer.Len() > 0 {
				l.docBuffer.WriteByte('\n') // Add newline separator
			}
			l.docBuffer.WriteString(lineContent) // Add the raw line content
			l.pos = lineEnd                      // Move past the line content
			// Consume the newline character to move to the next line
			if l.pos < len(l.input) && l.input[l.pos] == '\n' {
				l.pos++
				l.line++
				//		lineStart = l.pos // Update start for next potential line
			} else if l.pos >= len(l.input) { // EOF while collecting
				l.pos = l.startPos
				l.line = startLine // Reset for error
				l.Error("EOF reached while parsing docstring (missing END?)")
				return 0 // EOF
			}
		}
	}
	// Should be unreachable if EOF check inside loop works
	return 0
}

// Helper for identifier characters (needed if not accessible from parser utils)
func isalnum_(r rune) bool {
	return unicode.IsLetter(r) || unicode.IsDigit(r) || r == '_'
}

// SetResult allows the parser (via actions) to store the final parsed result
func (l *lexer) SetResult(res []Procedure) {
	l.result = res
}
